{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "df = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases'\n",
    "                 + '/iris/iris.data', header=None)\n",
    "\n",
    "\n",
    "df = df.sample(frac=1) # shuffle\n",
    "\n",
    "# add label indices column , i.e convert flower label to number\n",
    "mapping = {k: v for v, k in enumerate(df[4].unique())}\n",
    "df[5] = df[4].map(mapping)\n",
    "\n",
    "\n",
    "# normalise the data\n",
    "alldata = torch.tensor(df.iloc[:, [0,1,2,3]].values, dtype=torch.float)\n",
    "alldata = (alldata - alldata.mean(dim=0)) / alldata.var(dim=0)\n",
    "\n",
    "# create datasets\n",
    "\n",
    "target_tr = torch.tensor(df.iloc[:100, 5].values, dtype=torch.long)\n",
    "target_va = torch.tensor(df.iloc[100:, 5].values, dtype=torch.long)\n",
    "data_tr = alldata[:100]\n",
    "data_va = alldata[100:]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "\n",
    "def mlp(X, y, lr=0.01, num_epochs = 100):\n",
    "    \n",
    "    W1 = torch.randn(4,12, requires_grad=True)\n",
    "    W2 = torch.randn(12,3, requires_grad=True)\n",
    "    b1 = torch.tensor(0.0, requires_grad=True)\n",
    "    b2 = torch.tensor(0.0, requires_grad=True)\n",
    "    \n",
    "    for x in range(num_epochs):\n",
    "        \n",
    "        W1.grad = None\n",
    "        W2.grad = None\n",
    "        b1.grad = None\n",
    "        b2.grad = None\n",
    "        \n",
    "        logits = torch.relu(X @ W1 + b1) @ W2 + b2\n",
    "        error = torch.nn.functional.cross_entropy(logits, y)\n",
    "        #print(f\"Error  at {x} is {error}\")\n",
    "        error.backward()\n",
    "        \n",
    "        w1 = W1.data - W1.grad*lr\n",
    "        W1.data = w1\n",
    "        \n",
    "        w2 = W2.data - W2.grad*lr\n",
    "        W2.data = w2\n",
    "        \n",
    "        B1 = b1 - b1.grad*lr\n",
    "        b1.data = B1\n",
    "        B2 = b2 - b2.grad*lr\n",
    "        b2.data = B2\n",
    "    \n",
    "    print(f\"Training Error is {error}\")\n",
    "    return  W1, W2, b1, b2, error\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training Error is 0.5068268179893494\n",
      "Validation Error is 0.6615543365478516\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "W1, W2, b1, b2, error = mlp(data_tr, target_tr)\n",
    "logits = torch.relu(data_va @ W1 + b1) @ W2 + b2\n",
    "error = torch.nn.functional.cross_entropy(logits, target_va)\n",
    "print(f\"Validation Error is {error}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "\n",
    "def getAccuracy(logits, targets):\n",
    "    \n",
    "    _, predicted_tr = torch.max(logits.data, 1)\n",
    "    accuracy_tr = (predicted_tr == targets).sum().data.numpy()/predicted_tr.shape\n",
    "    return accuracy_tr\n",
    "    \n",
    "    # predictions = []\n",
    "    # for i in range(logits.shape[0]):\n",
    "    #     _, index = torch.max(logits[i],0)\n",
    "    #     predictions.append(index)\n",
    "    \n",
    "    #score = accuracy_score(targets, predictions)\n",
    "    \n",
    "    #return score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training Error is 0.6117843389511108\n",
      "Training Error is 0.649422824382782\n",
      "Training Error is 0.4925763010978699\n",
      "Training Error is 0.5316982865333557\n",
      "Training Error is 0.6342013478279114\n",
      "Training Error is 0.7649069428443909\n",
      "Training Error is 0.5610867142677307\n",
      "Training Error is 0.6748521327972412\n",
      "Training Error is 0.45868685841560364\n",
      "Training Error is 0.5051885843276978\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<matplotlib.legend.Legend at 0x7f95b4553390>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 37
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "train_error = np.zeros(10)\n",
    "val_error = np.zeros(10)\n",
    "\n",
    "for x in range(10):\n",
    "    W1, W2, b1, b2, error = mlp(data_tr, target_tr)\n",
    "    train_error[x] = error\n",
    "    logits = torch.relu(data_va @ W1 + b1) @ W2 + b2\n",
    "    error = torch.nn.functional.cross_entropy(logits, target_va)\n",
    "    val_error[x] = error\n",
    "\n",
    "x = np.linspace(1,10,10)\n",
    "plt.plot(x, train_error, label=\"training error\") \n",
    "plt.plot(x, val_error, label=\"validation error\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training Error is 0.43027064204216003\n",
      "0.85\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "W1, W2, b1, b2 ,_ = mlp(data_tr, target_tr)\n",
    "logits = torch.relu(data_tr @ W1 + b1) @ W2 + b2\n",
    "score  = getAccuracy(logits, target_tr)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Training Error is 0.4551851153373718\n",
      "Training Error is 0.7915902137756348\n",
      "Training Error is 0.4932372272014618\n",
      "Training Error is 0.7178360819816589\n",
      "Training Error is 0.537736713886261\n",
      "Training Error is 0.3295105993747711\n",
      "Training Error is 0.7705748677253723\n",
      "Training Error is 0.6578662395477295\n",
      "Training Error is 0.3412044942378998\n",
      "Training Error is 0.5136879086494446\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "%matplotlib qt\n",
    "train_score = np.zeros(10)\n",
    "val_score = np.zeros(10)\n",
    "\n",
    "for x in range(10):\n",
    "    W1, W2, b1, b2, error = mlp(data_tr, target_tr)\n",
    "    \n",
    "    logits = torch.relu(data_tr @ W1 + b1) @ W2 + b2\n",
    "    score = getAccuracy(logits, target_tr)\n",
    "    train_score[x] = score\n",
    "    \n",
    "    logits_2 = torch.relu(data_va @ W1 + b1) @ W2 + b2\n",
    "    score = getAccuracy(logits_2, target_va)\n",
    "    val_score[x] = score\n",
    "\n",
    "x = np.linspace(1,10,10)\n",
    "plt.plot(x, train_score, label=\"Training Accuracy\") \n",
    "plt.plot(x, train_error, label=\"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(\"MLP_accuracy.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# def MLP_2(data_tr, data_va, targets_tr, targets_va, epochs = 100, lr = 0.01):\n",
    "#   W1 = torch.randn((4,12), requires_grad=True, dtype= torch.float)\n",
    "#   W2 = torch.randn((12,3), requires_grad=True, dtype= torch.float)\n",
    "#   b1 = torch.tensor(0.0, requires_grad=True)\n",
    "#   b2 = torch.tensor(0.0, requires_grad=True)\n",
    "# \n",
    "#   for _ in range(epochs):\n",
    "#     logits = torch.relu(data_tr @ W1 + b1) @ W2 + b2\n",
    "#     loss = torch.nn.functional.cross_entropy(logits, targets_tr)\n",
    "#     loss.backward()\n",
    "# \n",
    "#     z_W1 = W1 - W1.grad*lr\n",
    "#     z_W2 = W2 - W2.grad*lr\n",
    "#     z_b1 = b1 - b1.grad*lr\n",
    "#     z_b2 = b2 - b2.grad*lr\n",
    "# \n",
    "#     W1.data = z_W1\n",
    "#     W2.data = z_W2\n",
    "#     b1.data = z_b1\n",
    "#     b2.data = z_b2\n",
    "#       \n",
    "#     #Clears old gradients\n",
    "#     W1.grad.detach().zero_()\n",
    "#     W2.grad.detach().zero_()\n",
    "#     b1.grad.detach().zero_()\n",
    "#     b2.grad.detach().zero_()\n",
    "# \n",
    "# \n",
    "#   W1, W2, b1, b2 ,_ = mlp(data_tr, target_tr) \n",
    "# \n",
    "#   predicted = torch.relu(data_tr @ W1 + b1) @ W2 + b2\n",
    "#   #_, predicted_tr = torch.max(predicted.data, 1)\n",
    "#   #accuracy_tr = (predicted_tr == targets_tr).sum().data.numpy()/predicted_tr.shape\n",
    "#   score = getAccuracy(predicted,targets_tr)\n",
    "#   print(score)\n",
    "# \n",
    "# \n",
    "#   predicted2 = torch.relu(data_va @ W1 + b1) @ W2 + b2\n",
    "#   #_, predicted_va = torch.max(predicted2.data, 1)\n",
    "#   #accuracy_va = (predicted_va == targets_va).sum().data.numpy()/predicted_va.shape\n",
    "#   score_2 = getAccuracy(predicted2,targets_va)\n",
    "#   print(score_2)\n",
    "# \n",
    "#   return score, score_2\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n",
    "# train_acc = np.empty(20)\n",
    "# test_acc = np.empty(20)\n",
    "# iterations = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
    "# \n",
    "# for i in range(20):\n",
    "#   accuracy_tr, accuracy_va = MLP_2(data_tr, data_va, target_tr, target_va)\n",
    "#   train_acc[i] = accuracy_tr\n",
    "#   test_acc[i] = accuracy_va\n",
    "# \n",
    "# plt.plot(iterations, train_acc, label = \"Train\")\n",
    "# plt.plot(iterations, test_acc, label = \"Validation\")\n",
    "# plt.xlabel(\"Run number\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# #plt.xticks(np.arange(1, 21, step=1))\n",
    "# plt.legend()\n",
    "# plt.title(\"Accuracy on validation and train data\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}